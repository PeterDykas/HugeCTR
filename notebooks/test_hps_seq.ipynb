{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af74fa74",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Hierarchical Parameter Server Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e01767",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In HugeCTR version 3.5, we provide Python APIs for embedding table lookup with [HugeCTR Hierarchical Parameter Server (HPS)](https://nvidia-merlin.github.io/HugeCTR/master/hugectr_core_features.html#hierarchical-parameter-server)\n",
    "HPS supports different database backends and GPU embedding caches.\n",
    "\n",
    "This notebook demonstrates how to use HPS with HugeCTR Python APIs. Without loss of generality, the HPS APIs are utilized together with the ONNX Runtime APIs to create an ensemble inference model, where HPS is responsible for embedding table lookup while the ONNX model takes charge of feed forward of dense neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600046ad",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "### Get HugeCTR from NGC\n",
    "\n",
    "The HugeCTR Python module is preinstalled in the 22.05 and later [Merlin Training Container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-training): `nvcr.io/nvidia/merlin/merlin-training:22.05`.\n",
    "\n",
    "You can check the existence of required libraries by running the following Python code after launching this container.\n",
    "\n",
    "```bash\n",
    "$ python3 -c \"import hugectr\"\n",
    "```\n",
    "\n",
    "**Note**: This Python module contains both training APIs and offline inference APIs. For online inference with Triton, please refer to [HugeCTR Backend](https://github.com/triton-inference-server/hugectr_backend).\n",
    "\n",
    "> If you prefer to build HugeCTR from the source code instead of using the NGC container, please refer to the\n",
    "> [How to Start Your Development](https://nvidia-merlin.github.io/HugeCTR/master/hugectr_contributor_guide.html#how-to-start-your-development)\n",
    "> documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6dd9bf",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "HugeCTR provides a tool to generate synthetic datasets. The [Data Generator](https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html#data-generator-api) is capable of generating datasets of different file formats and different distributions. We will generate one-hot Parquet datasets with power-law distribution for this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608001fb",
   "metadata": {},
   "source": [
    "## Train from Scratch\n",
    "\n",
    "We can train fom scratch by performing the following steps with Python APIs:\n",
    "\n",
    "1. Create the solver, reader and optimizer, then initialize the model.\n",
    "2. Construct the model graph by adding input, sparse embedding and dense layers in order.\n",
    "3. Compile the model and have an overview of the model graph.\n",
    "4. Dump the model graph to the JSON file.\n",
    "5. Fit the model, save the model weights and optimizer states implicitly.\n",
    "6. Dump one batch of evaluation results to files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8efb931",
   "metadata": {},
   "source": [
    "## Convert HugeCTR to ONNX\n",
    "\n",
    "We will convert the saved HugeCTR models to ONNX using the HugeCTR to ONNX Converter. For more information about the converter, refer to the README in the [onnx_converter](https://github.com/NVIDIA-Merlin/HugeCTR/tree/master/onnx_converter) directory of the repository.\n",
    "\n",
    "For the sake of double checking the correctness, we will investigate both cases of conversion depending on whether or not to convert the sparse embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a21a3009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting torch\n",
      "  Downloading torch-1.11.0-cp38-cp38-manylinux1_x86_64.whl (750.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.12.0-cp38-cp38-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading torchaudio-0.11.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.26.9)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-1.11.0 torchaudio-0.11.0 torchvision-0.12.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1007431f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HCTR][23:09:08.394][WARNING][RK0][main]: default_value_for_each_table.size() is not equal to the number of embedding tables\n",
      "[HCTR][23:09:08.394][WARNING][RK0][main]: default_value_for_each_table.size() is not equal to the number of embedding tables\n",
      "[HCTR][23:09:08.394][INFO][RK0][main]: Creating ParallelHashMap CPU database backend...\n",
      "[HCTR][23:09:08.394][INFO][RK0][main]: Created parallel (16 partitions) blank database backend in local memory!\n",
      "[HCTR][23:09:08.394][INFO][RK0][main]: Volatile DB: initial cache rate = 1\n",
      "[HCTR][23:09:08.394][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0\n",
      "[HCTR][23:09:08.395][INFO][RK0][main]: Table: hps_et.hps_demo.sparse_embedding1; cached 1000 / 1000 embeddings in volatile database (ParallelHashMap); load: 1000 / 18446744073709551615 (0.00%).\n",
      "[HCTR][23:09:08.396][INFO][RK0][main]: Table: hps_et.hps_demo.sparse_embedding2; cached 1000 / 1000 embeddings in volatile database (ParallelHashMap); load: 1000 / 18446744073709551615 (0.00%).\n",
      "[HCTR][23:09:08.397][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][23:09:08.398][INFO][RK0][main]: Table: hps_et.hps_demo1.sparse_embedding1; cached 1000 / 1000 embeddings in volatile database (ParallelHashMap); load: 1000 / 18446744073709551615 (0.00%).\n",
      "[HCTR][23:09:08.399][INFO][RK0][main]: Table: hps_et.hps_demo1.sparse_embedding2; cached 1000 / 1000 embeddings in volatile database (ParallelHashMap); load: 1000 / 18446744073709551615 (0.00%).\n",
      "[HCTR][23:09:08.399][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][23:09:08.399][INFO][RK0][main]: Create embedding cache in device 0.\n",
      "[HCTR][23:09:08.403][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 0.500000\n",
      "[HCTR][23:09:08.403][INFO][RK0][main]: Configured cache hit rate threshold: 1.000000\n",
      "[HCTR][23:09:08.404][INFO][RK0][main]: Create embedding cache in device 1.\n",
      "[HCTR][23:09:08.406][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 0.500000\n",
      "[HCTR][23:09:08.406][INFO][RK0][main]: Configured cache hit rate threshold: 1.000000\n",
      "[HCTR][23:09:08.408][INFO][RK0][main]: Create inference session on device: 07f6cf2410000\n",
      "<class 'int'>\n",
      "7f6cf2400000\n",
      "<class 'int'>\n",
      "\n",
      "[HCTR][23:09:08.408][INFO][RK0][main]: Model name: hps_demo\n",
      "[HCTR][23:09:08.408][INFO][RK0][main]: Number of embedding tables: 2\n",
      "[HCTR][23:09:08.408][INFO][RK0][main]: Use I64 input key: True\n",
      "[HCTR][23:09:08.409][INFO][RK0][main]: Create inference session on device: 1\n",
      "[HCTR][23:09:08.409][INFO][RK0][main]: Model name: hps_demo1\n",
      "[HCTR][23:09:08.409][INFO][RK0][main]: Number of embedding tables: 2\n",
      "[HCTR][23:09:08.409][INFO][RK0][main]: Use I64 input key: True\n",
      "tensor([[0.0000e+00, 1.0000e+00, 2.0000e+00,  ..., 1.2500e+02, 1.2600e+02,\n",
      "         1.2700e+02],\n",
      "        [1.2800e+02, 1.2900e+02, 1.3000e+02,  ..., 2.5300e+02, 2.5400e+02,\n",
      "         2.5500e+02],\n",
      "        [2.5600e+02, 2.5700e+02, 2.5800e+02,  ..., 3.8100e+02, 3.8200e+02,\n",
      "         3.8300e+02],\n",
      "        ...,\n",
      "        [7.8080e+03, 7.8090e+03, 7.8100e+03,  ..., 7.9330e+03, 7.9340e+03,\n",
      "         7.9350e+03],\n",
      "        [7.9360e+03, 7.9370e+03, 7.9380e+03,  ..., 8.0610e+03, 8.0620e+03,\n",
      "         8.0630e+03],\n",
      "        [8.0640e+03, 8.0650e+03, 8.0660e+03,  ..., 8.1890e+03, 8.1900e+03,\n",
      "         8.1910e+03]], device='cuda:0')\n",
      "tensor([[ 8192.,  8193.,  8194.,  ...,  8317.,  8318.,  8319.],\n",
      "        [ 8320.,  8321.,  8322.,  ...,  8445.,  8446.,  8447.],\n",
      "        [ 8448.,  8449.,  8450.,  ...,  8573.,  8574.,  8575.],\n",
      "        ...,\n",
      "        [16000., 16001., 16002.,  ..., 16125., 16126., 16127.],\n",
      "        [16128., 16129., 16130.,  ..., 16253., 16254., 16255.],\n",
      "        [16256., 16257., 16258.,  ..., 16381., 16382., 16383.]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Runtime error: an illegal memory access was encountered\n\tcudaStreamSynchronize(stream) at lookup(/workspace/HugeCTR/src/hps/embedding_cache.cpp:260)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 91>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m embd3_ptr \u001b[38;5;241m=\u001b[39m embedding3\u001b[38;5;241m.\u001b[39mdata_ptr()\n\u001b[1;32m     89\u001b[0m embd4_ptr \u001b[38;5;241m=\u001b[39m embedding4\u001b[38;5;241m.\u001b[39mdata_ptr()\n\u001b[0;32m---> 91\u001b[0m \u001b[43mhps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhps_demo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43membd3_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m hps\u001b[38;5;241m.\u001b[39mlookup(key4, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhps_demo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m,embd4_ptr)\n\u001b[1;32m     93\u001b[0m embedding3 \u001b[38;5;241m=\u001b[39m embedding3\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m128\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Runtime error: an illegal memory access was encountered\n\tcudaStreamSynchronize(stream) at lookup(/workspace/HugeCTR/src/hps/embedding_cache.cpp:260)"
     ]
    }
   ],
   "source": [
    "from hugectr.inference import HPS, ParameterServerConfig, InferenceParams\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import onnxruntime as ort\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "slot_size_array = [10000, 10000, 10000, 10000]\n",
    "key_offset = np.insert(np.cumsum(slot_size_array), 0, 0)[:-1]\n",
    "batch_size = 64\n",
    "\n",
    "# 1. Configure the HPS hyperparameters\n",
    "ps_config = ParameterServerConfig(\n",
    "           emb_table_name = {\"hps_demo\": [\"sparse_embedding1\", \"sparse_embedding2\"]},\n",
    "           embedding_vec_size = {\"hps_demo\": [128, 128]},\n",
    "           max_feature_num_per_sample_per_emb_table = {\"hps_demo\": [2, 2]},\n",
    "           inference_params_array = [\n",
    "              InferenceParams(\n",
    "                model_name = \"hps_demo\",\n",
    "                max_batchsize = batch_size,\n",
    "                hit_rate_threshold = 1.0,\n",
    "                dense_model_file = \"\",\n",
    "                sparse_model_files = [\"sequential.model\", \"sequential.model\"],\n",
    "                deployed_devices = [0,1],\n",
    "                use_gpu_embedding_cache = True,\n",
    "                cache_size_percentage = 0.5,\n",
    "                i64_input_key = True)\n",
    "           ])\n",
    "\n",
    "# 2. Initialize the HPS object\n",
    "hps = HPS(ps_config)\n",
    "\n",
    "# 3. Loading the Parquet data.\n",
    "# df = pd.read_parquet(\"data_parquet/val/gen_0.parquet\")\n",
    "# dense_input_columns = df.columns[1:11]\n",
    "# cat_input1_columns = df.columns[11:13]\n",
    "# cat_input2_columns = df.columns[13:15]\n",
    "# dense_input = df[dense_input_columns].loc[0:batch_size-1].to_numpy(dtype=np.float32)\n",
    "# cat_input1 = (df[cat_input1_columns].loc[0:batch_size-1].to_numpy(dtype=np.int64) + key_offset[0:2]).reshape((batch_size, 2, 1))\n",
    "# cat_input2 = (df[cat_input2_columns].loc[0:batch_size-1].to_numpy(dtype=np.int64) + key_offset[2:4]).reshape((batch_size, 2, 1))\n",
    "\n",
    "# 4. Make inference from the HPS object and the ONNX inference session of `hps_demo_without_embedding.onnx`.\n",
    "\n",
    "key1 = np.arange(0, batch_size, dtype=np.ulonglong)\n",
    "key2 =  np.arange(batch_size, batch_size * 2, dtype=np.ulonglong)\n",
    "\n",
    "embedding1 = torch.zeros(batch_size * 128).to(device)\n",
    "embedding2 = torch.zeros(batch_size * 128).to(device)\n",
    "embd1_ptr = embedding1.data_ptr()\n",
    "embd2_ptr = embedding2.data_ptr()\n",
    "\n",
    "print(\"{:x}\".format(embd1_ptr))\n",
    "print(type(embd1_ptr))\n",
    "print(\"{:x}\".format(embd2_ptr))\n",
    "print(type(embd2_ptr))\n",
    "\n",
    "hps.lookup(key1, \"hps_demo\", 0,embd1_ptr)\n",
    "hps.lookup(key2, \"hps_demo\", 1,embd2_ptr)\n",
    "embedding1 = embedding1.reshape(batch_size, 128)\n",
    "embedding2 = embedding2.reshape(batch_size, 128)\n",
    "\n",
    "print(embedding1)\n",
    "print(embedding2)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "key3 = np.arange(0, batch_size, dtype=np.ulonglong)\n",
    "key4 =  np.arange(batch_size, batch_size * 2, dtype=np.ulonglong)\n",
    "\n",
    "embedding3 = torch.zeros(batch_size * 128).to(device)\n",
    "embedding4 = torch.zeros(batch_size * 128).to(device)\n",
    "embd3_ptr = embedding3.data_ptr()\n",
    "embd4_ptr = embedding4.data_ptr()\n",
    "\n",
    "hps.lookup(key3, \"hps_demo\", 0,embd3_ptr)\n",
    "hps.lookup(key4, \"hps_demo\", 1,embd4_ptr)\n",
    "embedding3 = embedding3.reshape(batch_size, 128)\n",
    "embedding4 = embedding4.reshape(batch_size, 128)\n",
    "\n",
    "print(embedding3)\n",
    "print(embedding4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c1da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
