{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688314e5",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Hierarchical Parameter Server Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4607acac",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In HugeCTR version 3.5, we provide Python APIs for embedding table lookup with [HugeCTR Hierarchical Parameter Server (HPS)](https://nvidia-merlin.github.io/HugeCTR/master/hugectr_core_features.html#hierarchical-parameter-server)\n",
    "HPS supports different database backends and GPU embedding caches.\n",
    "\n",
    "This notebook demonstrates how to use HPS with HugeCTR Python APIs. Without loss of generality, the HPS APIs are utilized together with the ONNX Runtime APIs to create an ensemble inference model, where HPS is responsible for embedding table lookup while the ONNX model takes charge of feed forward of dense neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73398bee",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "### Get HugeCTR from NGC\n",
    "\n",
    "The HugeCTR Python module is preinstalled in the 22.05 and later [Merlin Training Container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-training): `nvcr.io/nvidia/merlin/merlin-training:22.05`.\n",
    "\n",
    "You can check the existence of required libraries by running the following Python code after launching this container.\n",
    "\n",
    "```bash\n",
    "$ python3 -c \"import hugectr\"\n",
    "```\n",
    "\n",
    "**Note**: This Python module contains both training APIs and offline inference APIs. For online inference with Triton, please refer to [HugeCTR Backend](https://github.com/triton-inference-server/hugectr_backend).\n",
    "\n",
    "> If you prefer to build HugeCTR from the source code instead of using the NGC container, please refer to the\n",
    "> [How to Start Your Development](https://nvidia-merlin.github.io/HugeCTR/master/hugectr_contributor_guide.html#how-to-start-your-development)\n",
    "> documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764aaa78",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "HugeCTR provides a tool to generate synthetic datasets. The [Data Generator](https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html#data-generator-api) is capable of generating datasets of different file formats and different distributions. We will generate one-hot Parquet datasets with power-law distribution for this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357b63c8",
   "metadata": {},
   "source": [
    "## Train from Scratch\n",
    "\n",
    "We can train fom scratch by performing the following steps with Python APIs:\n",
    "\n",
    "1. Create the solver, reader and optimizer, then initialize the model.\n",
    "2. Construct the model graph by adding input, sparse embedding and dense layers in order.\n",
    "3. Compile the model and have an overview of the model graph.\n",
    "4. Dump the model graph to the JSON file.\n",
    "5. Fit the model, save the model weights and optimizer states implicitly.\n",
    "6. Dump one batch of evaluation results to files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9f80b8",
   "metadata": {},
   "source": [
    "## Convert HugeCTR to ONNX\n",
    "\n",
    "We will convert the saved HugeCTR models to ONNX using the HugeCTR to ONNX Converter. For more information about the converter, refer to the README in the [onnx_converter](https://github.com/NVIDIA-Merlin/HugeCTR/tree/master/onnx_converter) directory of the repository.\n",
    "\n",
    "For the sake of double checking the correctness, we will investigate both cases of conversion depending on whether or not to convert the sparse embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a9b45c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HCTR][18:48:38.244][WARNING][RK0][main]: default_value_for_each_table.size() is not equal to the number of embedding tables\n",
      "[HCTR][18:48:38.244][INFO][RK0][main]: Creating ParallelHashMap CPU database backend...\n",
      "[HCTR][18:48:38.244][INFO][RK0][main]: Created parallel (16 partitions) blank database backend in local memory!\n",
      "[HCTR][18:48:38.244][INFO][RK0][main]: Volatile DB: initial cache rate = 1\n",
      "[HCTR][18:48:38.244][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0\n",
      "[HCTR][18:48:38.253][INFO][RK0][main]: Table: hps_et.hps_demo.sparse_embedding1; cached 15795 / 15795 embeddings in volatile database (ParallelHashMap); load: 15669 / 18446744073709551615 (0.00%).\n",
      "[HCTR][18:48:38.258][INFO][RK0][main]: Table: hps_et.hps_demo.sparse_embedding2; cached 15769 / 15769 embeddings in volatile database (ParallelHashMap); load: 15640 / 18446744073709551615 (0.00%).\n",
      "[HCTR][18:48:38.258][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][18:48:38.258][INFO][RK0][main]: Create embedding cache in device 0.\n",
      "[HCTR][18:48:38.261][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 0.500000\n",
      "[HCTR][18:48:38.261][INFO][RK0][main]: Configured cache hit rate threshold: 1.000000\n",
      "[HCTR][18:48:38.414][INFO][RK0][main]: Create inference session on device: 0\n",
      "[HCTR][18:48:38.414][INFO][RK0][main]: Model name: hps_demo\n",
      "[HCTR][18:48:38.414][INFO][RK0][main]: Number of embedding tables: 2\n",
      "[HCTR][18:48:38.414][INFO][RK0][main]: Use I64 input key: True\n",
      "7f3a3c800000\n",
      "<class 'int'>\n",
      "7f3a3c820000\n",
      "<class 'int'>\n",
      "tensor([[[-4.3081e-02,  5.0226e-02,  3.0220e-02,  ...,  1.3463e-02,\n",
      "           2.9480e-02, -1.2839e-02],\n",
      "         [-3.8070e-03, -9.8038e-03, -2.2841e-03,  ..., -1.8380e-02,\n",
      "          -1.9059e-02,  1.1620e-02]],\n",
      "\n",
      "        [[ 3.3203e-02, -1.0138e-01,  9.6000e-02,  ...,  3.5866e-02,\n",
      "           2.8645e-02, -2.2202e-02],\n",
      "         [ 3.9994e-02,  4.2667e-03, -8.9006e-03,  ...,  4.0737e-02,\n",
      "           2.2075e-02,  9.0898e-03]],\n",
      "\n",
      "        [[-6.1854e-03, -2.9899e-02, -2.4601e-02,  ...,  1.2887e-02,\n",
      "          -1.5822e-02, -2.9166e-04],\n",
      "         [-2.9233e-02,  2.8997e-03, -5.1137e-03,  ..., -3.2931e-02,\n",
      "           1.8204e-02, -2.5117e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.1854e-03, -2.9899e-02, -2.4601e-02,  ...,  1.2887e-02,\n",
      "          -1.5822e-02, -2.9166e-04],\n",
      "         [-5.1077e-03, -2.1900e-05,  1.2803e-02,  ...,  4.7786e-04,\n",
      "          -6.0078e-02,  7.0842e-03]],\n",
      "\n",
      "        [[-3.0293e-02,  2.3992e-03, -1.7653e-03,  ..., -8.5250e-03,\n",
      "          -6.6194e-03,  2.7050e-03],\n",
      "         [-1.5811e-02, -3.0828e-02,  2.5717e-03,  ..., -2.2000e-02,\n",
      "           3.3116e-02, -1.5603e-03]],\n",
      "\n",
      "        [[-2.0078e-02,  6.5638e-04,  3.0522e-02,  ...,  1.4937e-02,\n",
      "          -1.4094e-03, -1.1646e-02],\n",
      "         [-5.1077e-03, -2.1900e-05,  1.2803e-02,  ...,  4.7786e-04,\n",
      "          -6.0078e-02,  7.0842e-03]]], device='cuda:0')\n",
      "tensor([[[-0.0200,  0.0423,  0.0593,  ...,  0.0507,  0.0670, -0.0021],\n",
      "         [-0.0063,  0.0032,  0.0222,  ...,  0.0029,  0.0417, -0.0023]],\n",
      "\n",
      "        [[ 0.0601,  0.0368,  0.1351,  ..., -0.0859,  0.0661, -0.0183],\n",
      "         [-0.0143, -0.0551,  0.0374,  ...,  0.0171, -0.0327, -0.0209]],\n",
      "\n",
      "        [[ 0.0218,  0.0721, -0.0473,  ..., -0.0326, -0.0058,  0.0524],\n",
      "         [ 0.0065,  0.0238, -0.0122,  ..., -0.0303, -0.0272,  0.0204]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0689,  0.0166,  0.0041,  ...,  0.0806,  0.0474,  0.0435],\n",
      "         [ 0.0061, -0.0034,  0.0079,  ...,  0.0164,  0.0181,  0.0076]],\n",
      "\n",
      "        [[-0.0484, -0.0074,  0.0068,  ...,  0.0393,  0.0144,  0.0566],\n",
      "         [ 0.0118, -0.0107, -0.0310,  ...,  0.0111, -0.0044, -0.0169]],\n",
      "\n",
      "        [[-0.0285, -0.0280, -0.0363,  ...,  0.0365,  0.0407, -0.0049],\n",
      "         [-0.0183,  0.0372,  0.0379,  ...,  0.0313,  0.0287,  0.0449]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from hugectr.inference import HPS, ParameterServerConfig, InferenceParams\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import onnxruntime as ort\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "slot_size_array = [10000, 10000, 10000, 10000]\n",
    "key_offset = np.insert(np.cumsum(slot_size_array), 0, 0)[:-1]\n",
    "batch_size = 1024\n",
    "\n",
    "# 1. Configure the HPS hyperparameters\n",
    "ps_config = ParameterServerConfig(\n",
    "           emb_table_name = {\"hps_demo\": [\"sparse_embedding1\", \"sparse_embedding2\"]},\n",
    "           embedding_vec_size = {\"hps_demo\": [16, 32]},\n",
    "           max_feature_num_per_sample_per_emb_table = {\"hps_demo\": [2, 2]},\n",
    "           inference_params_array = [\n",
    "              InferenceParams(\n",
    "                model_name = \"hps_demo\",\n",
    "                max_batchsize = batch_size,\n",
    "                hit_rate_threshold = 1.0,\n",
    "                dense_model_file = \"\",\n",
    "                sparse_model_files = [\"hps_demo0_sparse_1000.model\", \"hps_demo1_sparse_1000.model\"],\n",
    "                deployed_devices = [0],\n",
    "                use_gpu_embedding_cache = True,\n",
    "                cache_size_percentage = 0.5,\n",
    "                i64_input_key = True)\n",
    "           ])\n",
    "\n",
    "# 2. Initialize the HPS object\n",
    "hps = HPS(ps_config)\n",
    "\n",
    "# 3. Loading the Parquet data.\n",
    "df = pd.read_parquet(\"data_parquet/val/gen_0.parquet\")\n",
    "dense_input_columns = df.columns[1:11]\n",
    "cat_input1_columns = df.columns[11:13]\n",
    "cat_input2_columns = df.columns[13:15]\n",
    "dense_input = df[dense_input_columns].loc[0:batch_size-1].to_numpy(dtype=np.float32)\n",
    "cat_input1 = (df[cat_input1_columns].loc[0:batch_size-1].to_numpy(dtype=np.int64) + key_offset[0:2]).reshape((batch_size, 2, 1))\n",
    "cat_input2 = (df[cat_input2_columns].loc[0:batch_size-1].to_numpy(dtype=np.int64) + key_offset[2:4]).reshape((batch_size, 2, 1))\n",
    "\n",
    "# 4. Make inference from the HPS object and the ONNX inference session of `hps_demo_without_embedding.onnx`.\n",
    "\n",
    "embedding1 = torch.zeros(batch_size* 2* 16).to(device)\n",
    "embedding2 = torch.zeros(batch_size* 2* 32).to(device)\n",
    "embd1_ptr = embedding1.data_ptr()\n",
    "embd2_ptr = embedding2.data_ptr()\n",
    "print(\"{:x}\".format(embd1_ptr))\n",
    "print(type(embd1_ptr))\n",
    "print(\"{:x}\".format(embd2_ptr))\n",
    "print(type(embd2_ptr))\n",
    "\n",
    "hps.lookup(cat_input1.flatten(), \"hps_demo\", 0,embd1_ptr)\n",
    "hps.lookup(cat_input2.flatten(), \"hps_demo\", 1,embd2_ptr)\n",
    "embedding1 = embedding1.reshape(batch_size, 2, 16)\n",
    "embedding2 = embedding2.reshape(batch_size, 2, 32)\n",
    "print(embedding1)\n",
    "print(embedding2)\n",
    "# sess = ort.InferenceSession(\"hps_demo_without_embedding.onnx\")\n",
    "# res = sess.run(output_names=[sess.get_outputs()[0].name],\n",
    "#                input_feed={sess.get_inputs()[0].name: dense_input,\n",
    "#                sess.get_inputs()[1].name: embedding1,\n",
    "#                sess.get_inputs()[2].name: embedding2})\n",
    "# pred = res[0]\n",
    "\n",
    "# # 5. Check the correctness by comparing with dumped evaluation results.\n",
    "# ground_truth = np.loadtxt(\"hps_demo_pred_1000\")\n",
    "# print(\"ground_truth: \", ground_truth)\n",
    "# diff = pred.flatten()-ground_truth\n",
    "# mse = np.mean(diff*diff)\n",
    "# print(\"pred: \", pred)\n",
    "# print(\"mse between pred and ground_truth: \", mse)\n",
    "\n",
    "# # 6. Make inference with the ONNX inference session of `hps_demo_with_embedding.onnx` (double check).\n",
    "# sess_ref = ort.InferenceSession(\"hps_demo_with_embedding.onnx\")\n",
    "# res_ref = sess_ref.run(output_names=[sess_ref.get_outputs()[0].name],\n",
    "#                    input_feed={sess_ref.get_inputs()[0].name: dense_input,\n",
    "#                    sess_ref.get_inputs()[1].name: cat_input1,\n",
    "#                    sess_ref.get_inputs()[2].name: cat_input2})\n",
    "# pred_ref = res_ref[0]\n",
    "# diff_ref = pred_ref.flatten()-ground_truth\n",
    "# mse_ref = np.mean(diff_ref*diff_ref)\n",
    "# print(\"pred_ref: \", pred_ref)\n",
    "# print(\"mse between pred_ref and ground_truth: \", mse_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23197a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
